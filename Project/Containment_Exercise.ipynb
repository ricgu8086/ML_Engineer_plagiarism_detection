{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Containment\n",
    "\n",
    "In this notebook, you'll implement a containment function that looks at a source and answer text and returns a *normalized* value that represents the similarity between those two texts based on their n-gram intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram counts\n",
    "\n",
    "One of the first things you'll need to do is to count up the occurrences of n-grams in your text data. To convert a set of text data into a matrix of counts, you can use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Below, you can set a value for n and use a CountVectorizer is used to count up the n-gram occurrences. In the next cell, we'll see that the CountVectorizer constructs a vocabulary, and later, we'll look at the matrix of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 5, 'is': 2, 'an': 0, 'answer': 1, 'text': 4, 'source': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "a_text = \"This is an answer text\"\n",
    "s_text = \"This is a source text\"\n",
    "\n",
    "# set n\n",
    "n = 1\n",
    "\n",
    "# instantiate an ngram counter\n",
    "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "# create a dictionary of n-grams by calling `.fit`\n",
    "vocab2int = counts.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of words:index\n",
    "print(vocab2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Create a vocabulary for 2-grams (aka \"bigrams\")\n",
    "\n",
    "Create a `CountVectorizer`, `counts_2grams`, and fit it to our text data. Print out the resultant vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 5, 'is an': 2, 'an answer': 0, 'answer text': 1, 'is source': 3, 'source text': 4}\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary for 2-grams\n",
    "n = 2\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "vocab2int = counts_2grams.fit([a_text, s_text]).vocabulary_\n",
    "print(vocab2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes up a word?\n",
    "\n",
    "You'll note that the word \"a\" does not appear in the vocabulary. And also that the words have been converted to lowercase. When `CountVectorizer` is passed `analyzer='word'` it defines a word as *two or more* characters and so it ignores uni-character words. In a lot of text analysis, single characters are often irrelevant to the meaning of a passage, so leaving them out of a vocabulary is often desired behavior. \n",
    "\n",
    "For our purposes, this default behavior will work well; we don't need uni-character words to determine cases of plagiarism, but you may still want to experiment with uni-character counts.\n",
    "\n",
    "> If you *do* want to include single characters as words, you can choose to do so by adding one more argument when creating the `CountVectorizer`; pass in the definition of a token, `token_pattern = r\"(?u)\\b\\w+\\b\"`. \n",
    "\n",
    "This regular expression defines a word as one or more characters. If you want to learn more about this vectorizer, I suggest reading through the [source code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L664), which is well documented.\n",
    "\n",
    "**Next, let's fit our `CountVectorizer` to all of our text data to make an array of n-gram counts!**\n",
    "\n",
    "The below code, assumes that `counts` is our `CountVectorizer` for the n-gram size we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 1]\n",
      " [0 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# create array of n-gram counts for the answer and source text\n",
    "ngrams = counts.fit_transform([a_text, s_text])\n",
    "\n",
    "# row = the 2 texts and column = indexed vocab terms (as mapped above)\n",
    "# ex. column 0 = 'an', col 1 = 'answer'.. col 4 = 'text'\n",
    "ngram_array = ngrams.toarray()\n",
    "print(ngram_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the top row indicates the n-gram counts for the answer text `a_text`, and the second row indicates those for the source text `s_text`. If they have n-grams in common, you can see this by looking at the column values. For example they both have one \"is\" (column 2) and \"text\" (column 4) and \"this\" (column 5).\n",
    "\n",
    "```\n",
    "[[1 1 1 0 1 1]    =   an  answer  [is]  ______  [text] [this]\n",
    " [0 0 1 1 1 1]]   =   __  ______  [is]  source  [text] [this]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Calculate containment values\n",
    "\n",
    "Assume your function takes in an `ngram_array` just like that generated above, for an answer text (row 0) and a source text (row 1). Using just this information, calculate the containment between the two texts. As before, it's okay to ignore the uni-character words.\n",
    "\n",
    "To calculate the containment:\n",
    "1. Calculate the n-gram **intersection** between the answer and source text.\n",
    "2. Add up the number of common terms.\n",
    "3. Normalize by dividing the value in step 2 by the number of n-grams in the answer text.\n",
    "\n",
    "The complete equation is:\n",
    "\n",
    "$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{S})}}{\\sum{count(\\text{ngram}_{A})}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containment(ngram_array):\n",
    "    ''' Containment is a measure of text similarity. It is the normalized, \n",
    "       intersection of ngram word counts in two texts.\n",
    "       :param ngram_array: an array of ngram counts for an answer and source text.\n",
    "       :return: a normalized containment value.'''\n",
    "    \n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    num = np.sum(np.logical_and(*ngram_array))\n",
    "    den = np.sum(ngram_array[0])\n",
    "    \n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment:  0.6\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test out your code\n",
    "containment_val = containment(ngrams.toarray())\n",
    "\n",
    "print('Containment: ', containment_val)\n",
    "\n",
    "# note that for the given texts, and n = 1\n",
    "# the containment value should be 3/5 or 0.6\n",
    "assert containment_val==0.6, 'Unexpected containment value for n=1.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment for n=2 :  0.25\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test for n = 2\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "bigram_counts = counts_2grams.fit_transform([a_text, s_text])\n",
    "\n",
    "# calculate containment\n",
    "containment_val = containment(bigram_counts.toarray())\n",
    "\n",
    "print('Containment for n=2 : ', containment_val)\n",
    "\n",
    "# the containment value should be 1/4 or 0.25\n",
    "assert containment_val==0.25, 'Unexpected containment value for n=2.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend trying out different phrases, and different values of n. What happens if you count for uni-character words? What if you make the sentences much larger?\n",
    "\n",
    "I find that the best way to understand a new concept is to think about how it might be applied in a variety of different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My tests\n",
    "\n",
    "(I'm going to follow the suggestions and make some tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = 'Abundantia was a divine personification of abundance and prosperity in ancient Rome. One explanation of the origin of the cornucopia myth, as related by Ovid, is that while the river god Achelous, in the form of a bull, was fighting Hercules, one of his horns was ripped off. The horn was taken up by the Naiads, who filled it with fruit and flowers, transforming it into a \"horn of plenty\", and gave it into Abundantia\\'s care. '\n",
    "answer_text = 'Abundantia was a divine personification of abundance and prosperity in ancient Rome. One explanation of the origin of the cornucopia myth is that while the river god Achelous, in the form of a bull, was fighting Hercules, one of his horns was ripped off.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_grams_vect = CountVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "one_gram_counts = one_grams_vect.fit_transform([answer_text, source_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abundantia': 1,\n",
       " 'was': 46,\n",
       " 'divine': 10,\n",
       " 'personification': 34,\n",
       " 'of': 29,\n",
       " 'abundance': 0,\n",
       " 'and': 4,\n",
       " 'prosperity': 36,\n",
       " 'in': 23,\n",
       " 'ancient': 3,\n",
       " 'rome': 40,\n",
       " 'one': 31,\n",
       " 'explanation': 11,\n",
       " 'the': 43,\n",
       " 'origin': 32,\n",
       " 'cornucopia': 9,\n",
       " 'myth': 27,\n",
       " 'is': 25,\n",
       " 'that': 42,\n",
       " 'while': 47,\n",
       " 'river': 39,\n",
       " 'god': 18,\n",
       " 'achelous': 2,\n",
       " 'form': 15,\n",
       " 'bull': 6,\n",
       " 'fighting': 12,\n",
       " 'hercules': 19,\n",
       " 'his': 20,\n",
       " 'horns': 22,\n",
       " 'ripped': 38,\n",
       " 'off': 30,\n",
       " 'as': 5,\n",
       " 'related': 37,\n",
       " 'by': 7,\n",
       " 'ovid': 33,\n",
       " 'horn': 21,\n",
       " 'taken': 41,\n",
       " 'up': 45,\n",
       " 'naiads': 28,\n",
       " 'who': 48,\n",
       " 'filled': 13,\n",
       " 'it': 26,\n",
       " 'with': 49,\n",
       " 'fruit': 16,\n",
       " 'flowers': 14,\n",
       " 'transforming': 44,\n",
       " 'into': 24,\n",
       " 'plenty': 35,\n",
       " 'gave': 17,\n",
       " 'care': 8}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_grams_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 2 0 1 0 1 0 5 1 2 1 0 1 0 1\n",
      "  0 1 1 1 0 1 4 0 0 3 1 0 0]\n",
      " [1 2 1 1 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 3 1 1 6 1 2 1 1 1 1 1\n",
      "  1 1 1 1 1 1 6 1 1 4 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(one_gram_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738095238095\n"
     ]
    }
   ],
   "source": [
    "# calculate containment\n",
    "containment_val = containment(one_gram_counts.toarray())\n",
    "print(containment_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_grams_vect = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "two_gram_counts = two_grams_vect.fit_transform([answer_text, source_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abundantia was': 2,\n",
       " 'was divine': 64,\n",
       " 'divine personification': 13,\n",
       " 'personification of': 47,\n",
       " 'of abundance': 37,\n",
       " 'abundance and': 0,\n",
       " 'and prosperity': 7,\n",
       " 'prosperity in': 49,\n",
       " 'in ancient': 27,\n",
       " 'ancient rome': 4,\n",
       " 'rome one': 53,\n",
       " 'one explanation': 43,\n",
       " 'explanation of': 14,\n",
       " 'of the': 41,\n",
       " 'the origin': 60,\n",
       " 'origin of': 45,\n",
       " 'the cornucopia': 56,\n",
       " 'cornucopia myth': 12,\n",
       " 'myth is': 35,\n",
       " 'is that': 31,\n",
       " 'that while': 55,\n",
       " 'while the': 68,\n",
       " 'the river': 61,\n",
       " 'river god': 52,\n",
       " 'god achelous': 21,\n",
       " 'achelous in': 3,\n",
       " 'in the': 28,\n",
       " 'the form': 57,\n",
       " 'form of': 18,\n",
       " 'of bull': 38,\n",
       " 'bull was': 9,\n",
       " 'was fighting': 65,\n",
       " 'fighting hercules': 15,\n",
       " 'hercules one': 22,\n",
       " 'one of': 44,\n",
       " 'of his': 39,\n",
       " 'his horns': 23,\n",
       " 'horns was': 26,\n",
       " 'was ripped': 66,\n",
       " 'ripped off': 51,\n",
       " 'myth as': 34,\n",
       " 'as related': 8,\n",
       " 'related by': 50,\n",
       " 'by ovid': 10,\n",
       " 'ovid is': 46,\n",
       " 'off the': 42,\n",
       " 'the horn': 58,\n",
       " 'horn was': 25,\n",
       " 'was taken': 67,\n",
       " 'taken up': 54,\n",
       " 'up by': 63,\n",
       " 'by the': 11,\n",
       " 'the naiads': 59,\n",
       " 'naiads who': 36,\n",
       " 'who filled': 69,\n",
       " 'filled it': 16,\n",
       " 'it with': 33,\n",
       " 'with fruit': 70,\n",
       " 'fruit and': 19,\n",
       " 'and flowers': 5,\n",
       " 'flowers transforming': 17,\n",
       " 'transforming it': 62,\n",
       " 'it into': 32,\n",
       " 'into horn': 30,\n",
       " 'horn of': 24,\n",
       " 'of plenty': 40,\n",
       " 'plenty and': 48,\n",
       " 'and gave': 6,\n",
       " 'gave it': 20,\n",
       " 'into abundantia': 29,\n",
       " 'abundantia care': 1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_grams_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0\n",
      "  1 1 1 0 2 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 0 1\n",
      "  1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(two_gram_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951219512195\n"
     ]
    }
   ],
   "source": [
    "# calculate containment\n",
    "containment_val = containment(two_gram_counts.toarray())\n",
    "print(containment_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-ngrams + 2-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_two_grams_vect = CountVectorizer(analyzer='word', ngram_range=(1,2))\n",
    "one_two_gram_counts = one_two_grams_vect.fit_transform([answer_text, source_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abundantia': 2,\n",
       " 'was': 110,\n",
       " 'divine': 23,\n",
       " 'personification': 81,\n",
       " 'of': 66,\n",
       " 'abundance': 0,\n",
       " 'and': 9,\n",
       " 'prosperity': 85,\n",
       " 'in': 50,\n",
       " 'ancient': 7,\n",
       " 'rome': 93,\n",
       " 'one': 74,\n",
       " 'explanation': 25,\n",
       " 'the': 99,\n",
       " 'origin': 77,\n",
       " 'cornucopia': 21,\n",
       " 'myth': 61,\n",
       " 'is': 56,\n",
       " 'that': 97,\n",
       " 'while': 115,\n",
       " 'river': 91,\n",
       " 'god': 39,\n",
       " 'achelous': 5,\n",
       " 'form': 33,\n",
       " 'bull': 15,\n",
       " 'fighting': 27,\n",
       " 'hercules': 41,\n",
       " 'his': 43,\n",
       " 'horns': 48,\n",
       " 'ripped': 89,\n",
       " 'off': 72,\n",
       " 'abundantia was': 4,\n",
       " 'was divine': 111,\n",
       " 'divine personification': 24,\n",
       " 'personification of': 82,\n",
       " 'of abundance': 67,\n",
       " 'abundance and': 1,\n",
       " 'and prosperity': 12,\n",
       " 'prosperity in': 86,\n",
       " 'in ancient': 51,\n",
       " 'ancient rome': 8,\n",
       " 'rome one': 94,\n",
       " 'one explanation': 75,\n",
       " 'explanation of': 26,\n",
       " 'of the': 71,\n",
       " 'the origin': 104,\n",
       " 'origin of': 78,\n",
       " 'the cornucopia': 100,\n",
       " 'cornucopia myth': 22,\n",
       " 'myth is': 63,\n",
       " 'is that': 57,\n",
       " 'that while': 98,\n",
       " 'while the': 116,\n",
       " 'the river': 105,\n",
       " 'river god': 92,\n",
       " 'god achelous': 40,\n",
       " 'achelous in': 6,\n",
       " 'in the': 52,\n",
       " 'the form': 101,\n",
       " 'form of': 34,\n",
       " 'of bull': 68,\n",
       " 'bull was': 16,\n",
       " 'was fighting': 112,\n",
       " 'fighting hercules': 28,\n",
       " 'hercules one': 42,\n",
       " 'one of': 76,\n",
       " 'of his': 69,\n",
       " 'his horns': 44,\n",
       " 'horns was': 49,\n",
       " 'was ripped': 113,\n",
       " 'ripped off': 90,\n",
       " 'as': 13,\n",
       " 'related': 87,\n",
       " 'by': 17,\n",
       " 'ovid': 79,\n",
       " 'horn': 45,\n",
       " 'taken': 95,\n",
       " 'up': 108,\n",
       " 'naiads': 64,\n",
       " 'who': 117,\n",
       " 'filled': 29,\n",
       " 'it': 58,\n",
       " 'with': 119,\n",
       " 'fruit': 35,\n",
       " 'flowers': 31,\n",
       " 'transforming': 106,\n",
       " 'into': 53,\n",
       " 'plenty': 83,\n",
       " 'gave': 37,\n",
       " 'care': 20,\n",
       " 'myth as': 62,\n",
       " 'as related': 14,\n",
       " 'related by': 88,\n",
       " 'by ovid': 18,\n",
       " 'ovid is': 80,\n",
       " 'off the': 73,\n",
       " 'the horn': 102,\n",
       " 'horn was': 47,\n",
       " 'was taken': 114,\n",
       " 'taken up': 96,\n",
       " 'up by': 109,\n",
       " 'by the': 19,\n",
       " 'the naiads': 103,\n",
       " 'naiads who': 65,\n",
       " 'who filled': 118,\n",
       " 'filled it': 30,\n",
       " 'it with': 60,\n",
       " 'with fruit': 120,\n",
       " 'fruit and': 36,\n",
       " 'and flowers': 10,\n",
       " 'flowers transforming': 32,\n",
       " 'transforming it': 107,\n",
       " 'it into': 59,\n",
       " 'into horn': 55,\n",
       " 'horn of': 46,\n",
       " 'of plenty': 70,\n",
       " 'plenty and': 84,\n",
       " 'and gave': 11,\n",
       " 'gave it': 38,\n",
       " 'into abundantia': 54,\n",
       " 'abundantia care': 3}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_two_grams_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0\n",
      "  1 1 1 0 2 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 0 1\n",
      "  1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(two_gram_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951219512195\n"
     ]
    }
   ],
   "source": [
    "# calculate containment\n",
    "containment_val = containment(two_gram_counts.toarray())\n",
    "print(containment_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmm... the value is exactly the same as in 2-grams case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
